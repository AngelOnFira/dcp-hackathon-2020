{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597524428556",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Installs\n",
    "\n",
    "pip install gitpython beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import requests, time\n",
    "from bs4 import BeautifulSoup\n",
    "import os, json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrape topic from Github\n",
    "\n",
    "topic_set = set()\n",
    "\n",
    "for i in range(7):\n",
    "    time.sleep(1)\n",
    "    page = requests.get(f\"https://github.com/topics?page={i}\")\n",
    "    # print(soup)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    for find in soup.find_all('a'):\n",
    "        topic = find['href']\n",
    "        if topic.startswith(\"/topics/\"):\n",
    "            topic_set.add(topic)\n",
    "\n",
    "    print(i, len(topic_set))\n",
    "\n",
    "print(\"Have topic set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrape repo names from topics - doesn't work\n",
    "topic_set = set()\n",
    "topic_set.add(\"/topics/cpp\")\n",
    "\n",
    "for topic in topic_set:\n",
    "    print(topic)\n",
    "    time.sleep(1)\n",
    "    page = requests.get(f\"https://github.com{topic}?o=desc&s=stars\")\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    for a in soup.find_all('a', {\"data-ga-click\":\"Explore, go to repository, location:explore feed\"}):\n",
    "        print(a['href'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get repos\n",
    "\n",
    "repos = [line.strip() for line in open('repos.txt')]\n",
    "\n",
    "repo_contrib_dict = defaultdict(list)\n",
    "\n",
    "for repo in repos:\n",
    "    repo_name = '{}/{}'.format(*repo.split(\"/\")[-2:])\n",
    "\n",
    "    stream = os.popen(f'''\n",
    "        rm -rf git-test > /dev/null;\n",
    "        git clone {repo} git-test > /dev/null;\n",
    "        cd git-test;\n",
    "        git log --all --format=\\'%aE\\' | sort -u\n",
    "    ''')\n",
    "    contributor_emails = stream.read().split(\"\\n\")[:-1]\n",
    "\n",
    "    for email in contributor_emails:\n",
    "        repo_contrib_dict[email].append(repo_name)\n",
    "\n",
    "    print(f\"Finished {repo}\")\n",
    "\n",
    "with open('contributors2.json', 'w') as outfile:\n",
    "    json.dump(repo_contrib_dict, outfile)\n",
    "\n",
    "print(\"All done\")"
   ]
  }
 ]
}