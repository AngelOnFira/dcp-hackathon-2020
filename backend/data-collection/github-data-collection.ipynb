{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1597572230666",
   "display_name": "Python 3.8.5 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Installs\n",
    "\n",
    "pip install gitpython beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import requests, time\n",
    "from bs4 import BeautifulSoup\n",
    "import os, json\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrape topic from Github\n",
    "\n",
    "topic_set = set()\n",
    "\n",
    "for i in range(7):\n",
    "    time.sleep(1)\n",
    "    page = requests.get(f\"https://github.com/topics?page={i}\")\n",
    "    # print(soup)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    for find in soup.find_all('a'):\n",
    "        topic = find['href']\n",
    "        if topic.startswith(\"/topics/\"):\n",
    "            topic_set.add(topic)\n",
    "\n",
    "    print(i, len(topic_set))\n",
    "\n",
    "print(\"Have topic set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scrape repo names from topics - doesn't work\n",
    "topic_set = set()\n",
    "topic_set.add(\"/topics/cpp\")\n",
    "\n",
    "for topic in topic_set:\n",
    "    print(topic)\n",
    "    time.sleep(1)\n",
    "    page = requests.get(f\"https://github.com{topic}?o=desc&s=stars\")\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    for a in soup.find_all('a', {\"data-ga-click\":\"Explore, go to repository, location:explore feed\"}):\n",
    "        print(a['href'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get repos\n",
    "\n",
    "repos = [line.strip() for line in open('npm-repos.txt')]\n",
    "\n",
    "repo_contrib_dict = defaultdict(list)\n",
    "\n",
    "for repo in repos:\n",
    "    repo_name = '{}/{}'.format(*repo.split(\"/\")[-2:])\n",
    "\n",
    "    stream = os.popen(f'''\n",
    "        rm -rf git-test > /dev/null;\n",
    "        git clone {repo} git-test > /dev/null;\n",
    "        cd git-test;\n",
    "        git log --all --format=\\'%aE\\' | sort -u\n",
    "    ''')\n",
    "    contributor_emails = stream.read().split(\"\\n\")[:-1]\n",
    "\n",
    "    for email in contributor_emails:\n",
    "        repo_contrib_dict[email].append(repo_name)\n",
    "\n",
    "    print(f\"Finished {repo}\")\n",
    "\n",
    "# Remove all users\n",
    "for key in [key for key in repo_contrib_dict if len(repo_contrib_dict[key]) == 1]: del repo_contrib_dict[key] \n",
    "\n",
    "with open('contributors-repos.json', 'w') as outfile:\n",
    "    json.dump(repo_contrib_dict, outfile)\n",
    "\n",
    "print(\"All done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def explore_node_page(package):\n",
    "    page = requests.get(f\"https://www.npmjs.com{package}\")\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    repository = soup.find('h3', string=\"Repository\")\n",
    "    git_link = repository.parent.findNext('a')['href']\n",
    "    return git_link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get node repo list\n",
    "url = \"https://www.npmjs.com/browse/depended\"\n",
    "counter = len([line.strip() for line in open('npm-repos.txt')])\n",
    "print(f\"starting at {counter}\")\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_10_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/39.0.2171.95 Safari/537.36'}\n",
    "\n",
    "while (counter < 1000):\n",
    "    page = requests.get(f\"https://www.npmjs.com/browse/depended?offset={counter}\", headers=headers)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "\n",
    "    for a in soup.find_all('a'):\n",
    "        if a['href'].startswith(\"/package/\"):\n",
    "            time.sleep(0.5)\n",
    "            package = a['href']\n",
    "            print(f\"Working on {package}\")\n",
    "            counter += 1\n",
    "            git_repo = explore_node_page(package)\n",
    "\n",
    "            if git_repo not in [line.strip() for line in open('npm-repos.txt')]:\n",
    "                with open('npm-repos.txt', 'a') as outfile:\n",
    "                    outfile.write(git_repo + '\\n')\n",
    "            else:\n",
    "                print(f\"duplicate url: {git_repo}\")\n",
    "\n",
    "    git_repo_list = []"
   ]
  }
 ]
}